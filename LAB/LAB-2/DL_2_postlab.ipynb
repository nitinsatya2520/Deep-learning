{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Design model to predict credit card risk data using multi-layer perceptron. Display weights in each layer."
      ],
      "metadata": {
        "id": "RrAlYmt5vL5l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8yo6WR5vEeK",
        "outputId": "9e607e83-3e86-4fbd-f8c6-a5546c0df422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 3.223619121941664\n",
            "Epoch 100: Loss = 3.223619121941664\n",
            "Epoch 200: Loss = 3.223619121941664\n",
            "Epoch 300: Loss = 3.223619121941664\n",
            "Epoch 400: Loss = 3.223619121941664\n",
            "Epoch 500: Loss = 3.223619121941664\n",
            "Epoch 600: Loss = 3.223619121941664\n",
            "Epoch 700: Loss = 3.223619121941664\n",
            "Epoch 800: Loss = 3.223619121941664\n",
            "Epoch 900: Loss = 3.223619121941664\n",
            "Layer 1 weights (W1):\n",
            "[[ 0.01059936  0.00617006  0.00683569 -0.01365956  0.01211944]\n",
            " [ 0.00261251 -0.00369277  0.00143388 -0.01776235  0.00408653]\n",
            " [-0.01029372 -0.0135267  -0.01522359  0.01112688 -0.00629263]]\n",
            "\n",
            "Layer 2 weights (W2):\n",
            "[[ 0.01533728 -0.00535801 -0.01707358 -0.01116524]\n",
            " [ 0.01235812 -0.00155898 -0.00548287  0.00160018]\n",
            " [ 0.00501783  0.01117399  0.01448499 -0.00359769]\n",
            " [-0.01326048 -0.00413465  0.00260281 -0.00963759]\n",
            " [-0.00957151  0.00343788 -0.00048652  0.00032797]]\n",
            "\n",
            "Layer 3 weights (W3):\n",
            "[[-0.00758495]\n",
            " [-0.00230401]\n",
            " [-0.00924233]\n",
            " [ 0.00890198]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# 1. Generate Synthetic Credit Card Risk Data\n",
        "def generate_data(n_samples=1000):\n",
        "    np.random.seed(42)\n",
        "    X = np.random.rand(n_samples, 3)  # 3 features (e.g., income, age, loan amount)\n",
        "    # Target (0 = low risk, 1 = high risk)\n",
        "    Y = (X[:, 0] + X[:, 1] - X[:, 2] > 1).astype(int).reshape(-1, 1)\n",
        "    return X, Y\n",
        "# 2. Initialize the parameters for the MLP\n",
        "def initialize_parameters(layer_sizes):\n",
        "    parameters = {}\n",
        "    parameters['W1'] = np.random.randn(layer_sizes[0], layer_sizes[1]) * 0.01\n",
        "    parameters['b1'] = np.zeros((1, layer_sizes[1]))\n",
        "    parameters['W2'] = np.random.randn(layer_sizes[1], layer_sizes[2]) * 0.01\n",
        "    parameters['b2'] = np.zeros((1, layer_sizes[2]))\n",
        "    parameters['W3'] = np.random.randn(layer_sizes[2], layer_sizes[3]) * 0.01\n",
        "    parameters['b3'] = np.zeros((1, layer_sizes[3]))\n",
        "    return parameters\n",
        "# 3. Forward pass\n",
        "def forward(X, parameters):\n",
        "    cache = {}\n",
        "    A = X\n",
        "    cache['A0'] = A\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "    for l in range(1, L+1):\n",
        "        Z = np.dot(A, parameters['W' + str(l)]) + parameters['b' + str(l)]\n",
        "        A = np.maximum(0, Z)  # ReLU activation\n",
        "        cache['A' + str(l)] = A\n",
        "        cache['Z' + str(l)] = Z\n",
        "\n",
        "    return A, cache\n",
        "# 4. Compute loss (Binary Cross-Entropy)\n",
        "def compute_loss(Y, A):\n",
        "    m = Y.shape[0]\n",
        "    loss = -np.mean(Y * np.log(A + 1e-8) + (1 - Y) * np.log(1 - A + 1e-8))\n",
        "    return loss\n",
        "# 5. Backward pass\n",
        "def backward(X, Y, parameters, cache):\n",
        "    gradients = {}\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "    m = X.shape[0]\n",
        "    # Gradient of output layer\n",
        "    dA = -(Y / (cache['A' + str(L)] + 1e-8)) + (1 - Y) / (1 - cache['A' + str(L)] + 1e-8)\n",
        "    dZ = dA * (cache['Z' + str(L)] > 0)  # ReLU derivative\n",
        "    gradients['dW' + str(L)] = np.dot(cache['A' + str(L-1)].T, dZ) / m\n",
        "    gradients['db' + str(L)] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "    for l in reversed(range(1, L)):\n",
        "        dA = np.dot(dZ, parameters['W' + str(l+1)].T)\n",
        "        dZ = dA * (cache['Z' + str(l)] > 0)  # ReLU derivative\n",
        "        # The error was in the next line. We need to transpose X, not dZ\n",
        "        gradients['dW' + str(l)] = np.dot(cache['A' + str(l-1)].T, dZ) / m\n",
        "        gradients['db' + str(l)] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "    return gradients\n",
        "# 6. Update parameters\n",
        "def update_parameters(parameters, gradients, learning_rate=0.01):\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(1, L+1):\n",
        "        parameters['W' + str(l)] -= learning_rate * gradients['dW' + str(l)]\n",
        "        parameters['b' + str(l)] -= learning_rate * gradients['db' + str(l)]\n",
        "    return parameters\n",
        "# 7. Train the MLP model\n",
        "def train_model(X, Y, layer_sizes, learning_rate=0.01, epochs=1000):\n",
        "    parameters = initialize_parameters(layer_sizes)\n",
        "    for i in range(epochs):\n",
        "        # Forward pass\n",
        "        A, cache = forward(X, parameters)\n",
        "        # Compute loss\n",
        "        loss = compute_loss(Y, A)\n",
        "        # Backward pass\n",
        "        gradients = backward(X, Y, parameters, cache)\n",
        "        # Update parameters\n",
        "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch {i}: Loss = {loss}')\n",
        "    return parameters\n",
        "# 8. Display the weights of each layer\n",
        "def display_weights(parameters):\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(1, L+1):\n",
        "        print(f'Layer {l} weights (W{l}):\\n{parameters[\"W\" + str(l)]}\\n')\n",
        "# 9. Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate synthetic data\n",
        "    X, Y = generate_data()\n",
        "    # Define the architecture\n",
        "    layer_sizes = [3, 5, 4, 1]  # 3 input features, 5 in hidden layer 1, 4 in hidden layer 2, 1 output\n",
        "    # Train the model\n",
        "    parameters = train_model(X, Y, layer_sizes, learning_rate=0.01, epochs=1000)\n",
        "    # Display the weights\n",
        "    display_weights(parameters)\n"
      ]
    }
  ]
}