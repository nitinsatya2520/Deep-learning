{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Design and implement a deep neural network for regression using the PyTorch. Define the model architecture, compile the model, and prepare it for training."
      ],
      "metadata": {
        "id": "TxQS6W7rzq62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VUskhHDzaeD",
        "outputId": "f4b537ec-8239-4703-b252-69c19b57e738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/100, Loss: 1.0097\n",
            "Epoch 10/100, Loss: 0.9790\n",
            "Epoch 20/100, Loss: 0.9777\n",
            "Epoch 30/100, Loss: 0.9785\n",
            "Epoch 40/100, Loss: 0.9781\n",
            "Epoch 50/100, Loss: 0.9782\n",
            "Epoch 60/100, Loss: 0.9784\n",
            "Epoch 70/100, Loss: 0.9780\n",
            "Epoch 80/100, Loss: 0.9787\n",
            "Epoch 90/100, Loss: 0.9780\n",
            "Test Loss: 1.0880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([200])) that is different to the input size (torch.Size([200, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.features[index], self.targets[index]\n",
        "# 1. Load dataset (replace with your actual dataset path)\n",
        "def load_dataset():\n",
        "    np.random.seed(42)\n",
        "    data = np.random.randn(1000, 5)  # 1000 samples, 5 features\n",
        "    target = 2 * data[:, 0] + 3 * data[:, 1] - data[:, 2] + np.random.randn(1000) * 0.5  # Synthetic target\n",
        "    return pd.DataFrame(data, columns=[f\"feature_{i}\" for i in range(5)]), target\n",
        "# 2. Handle missing values\n",
        "def handle_missing_values(df):\n",
        "    df = df.fillna(df.mean())\n",
        "    return df\n",
        "# 3. Normalize data\n",
        "def normalize_data(df, target):\n",
        "    scaler = StandardScaler()\n",
        "    df_normalized = scaler.fit_transform(df)\n",
        "    target_normalized = (target - np.mean(target)) / np.std(target)\n",
        "    return df_normalized, target_normalized\n",
        "# 4. Define the Deep Neural Network model\n",
        "class DeepNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
        "        super(DeepNNModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "# 5. Training function\n",
        "def train_model(model, dataloader, criterion, optimizer, epochs=100):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for features, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}/{epochs}, Loss: {running_loss/len(dataloader):.4f}')\n",
        "# 6. Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess dataset\n",
        "    df, target = load_dataset()\n",
        "    df.iloc[::10, 0] = np.nan  # Introduce some missing values for demonstration\n",
        "    df = handle_missing_values(df)\n",
        "    features, targets = normalize_data(df, target)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_dataset = CustomDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    # Initialize the model, loss function, and optimizer\n",
        "    input_dim = X_train.shape[1]\n",
        "    hidden_dim1 = 64\n",
        "    hidden_dim2 = 32\n",
        "    output_dim = 1\n",
        "    model = DeepNNModel(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    # Train the model\n",
        "    train_model(model, train_loader, criterion, optimizer, epochs=100)\n",
        "    # Evaluate the model on test data\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "        predictions = model(X_test_tensor)\n",
        "        test_loss = criterion(predictions, y_test_tensor)\n",
        "        print(f'Test Loss: {test_loss.item():.4f}')"
      ]
    }
  ]
}