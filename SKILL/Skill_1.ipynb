{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S33kiKYLn_W6",
        "outputId": "87e78503-b623-40d5-83f7-32952cd8d952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights:\n",
            " [[0.36374888 0.78693396 0.09860513 0.994099  ]\n",
            " [0.77276428 0.25020595 0.293068   0.29951896]\n",
            " [0.19673218 0.8801598  0.17534082 0.20848323]\n",
            " [0.97652462 0.04021597 0.45066903 0.67568807]\n",
            " [0.32705924 0.06073069 0.48866919 0.52882   ]\n",
            " [0.45612968 0.96494585 0.22448164 0.12358223]\n",
            " [0.69540942 0.2418801  0.17986551 0.71292555]\n",
            " [0.00384178 0.22550784 0.43283492 0.44182561]\n",
            " [0.65353244 0.91060769 0.28128708 0.98813218]\n",
            " [0.15928405 0.84988021 0.80195709 0.19158605]]\n",
            "Biases:\n",
            " [0.81833184 0.90191215 0.42534678 0.09871125]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        # Initialize weights and biases with small random numbers\n",
        "        self.weights = np.random.rand(input_dim, output_dim)\n",
        "        self.biases = np.random.rand(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute the weighted sum (dot product) + biases\n",
        "        return np.dot(x, self.weights) + self.biases\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define dimensions\n",
        "    input_dim = 10  # 10-dimensional input\n",
        "    output_dim = 4  # 4 classes\n",
        "\n",
        "    # Create the Perceptron model\n",
        "    model = Perceptron(input_dim, output_dim)\n",
        "\n",
        "    # Print the weights and biases\n",
        "    print(\"Weights:\\n\", model.weights)\n",
        "    print(\"Biases:\\n\", model.biases)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
        "column_names = [f'feature_{i}' for i in range(1, 58)] + ['label']  # 57 features + 1 label\n",
        "data = pd.read_csv(url, header=None, names=column_names)\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop('label', axis=1)\n",
        "y = data['label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Output the model weights and biases\n",
        "print(\"Model Weights:\\n\", model.coef_)\n",
        "print(\"Model Bias (Intercept):\\n\", model.intercept_)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "_coZaMuJrPpO",
        "outputId": "b41b7aa1-1b38-4e0a-fdd8-017d10a456d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9225199131064447\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.93       804\n",
            "           1       0.93      0.88      0.90       577\n",
            "\n",
            "    accuracy                           0.92      1381\n",
            "   macro avg       0.92      0.92      0.92      1381\n",
            "weighted avg       0.92      0.92      0.92      1381\n",
            "\n",
            "Model Weights:\n",
            " [[-6.30484991e-02 -1.55966326e-01  5.37660271e-02  8.35457212e-01\n",
            "   3.92067019e-01  1.25056243e-01  8.22607753e-01  1.76094406e-01\n",
            "   2.76446896e-01  4.26478684e-02 -7.95107223e-02 -8.82724356e-02\n",
            "   3.74448680e-03  5.07602317e-02  3.80416999e-01  9.06563815e-01\n",
            "   4.46244506e-01  5.95168250e-02  1.88961167e-01  5.77098195e-01\n",
            "   2.99528950e-01  2.79711781e-01  8.79688578e-01  1.48830897e-01\n",
            "  -2.60683756e+00 -8.47713857e-01 -4.13631064e+00  2.06264349e-01\n",
            "  -8.98191292e-01 -3.14601187e-01 -1.74403616e-01 -4.41755944e-01\n",
            "  -6.41467780e-01  5.95893169e-02 -7.69932449e-01  4.25838667e-01\n",
            "  -1.06913489e-01 -1.66370628e-01 -3.85728563e-01 -1.65778263e-01\n",
            "  -1.52307880e+00 -1.15448276e+00 -2.13092932e-01 -1.40109780e+00\n",
            "  -8.09593894e-01 -1.09951983e+00 -1.60025418e-01 -9.19735273e-01\n",
            "  -3.05829494e-01 -6.60367841e-02 -9.07408667e-02  2.14153862e-01\n",
            "   1.04393556e+00  7.15214783e-01 -2.44407772e-01  8.12481135e-01\n",
            "   6.40743403e-01]]\n",
            "Model Bias (Intercept):\n",
            " [-2.67529625]\n"
          ]
        }
      ]
    }
  ]
}